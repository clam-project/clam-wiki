== First tries ==

Installing with ubuntu fiesty was relatively easy.  The biggest problems were not related to clam: getting the nvidia to work and I also had a mysterious problem with the audio that seemed to go away by itself one day.  I also tried installing clam on my Gentoo desktop, but I didn't get it working.  It could be due to the fact that I tried to install qt4 instead of just the libraries (gentoo doesn't like having more than one version of a given program at the same time) due to not reading the directions closely.  The next step in getting it working was figuring out what jack is used for...

== Playing with the vowel synthesizer ==

(May 21)
Meeting Xavier last week was very helpful.  I found out that my idea to use glottal pulses as an input for the vowel synthesizer was easier than I thought.  I didn't need to write any code and most of what I needed was already in the experimental vowel synthesizer made by David Garcia.  Learning how to use Jack was the missing link that let me play the electroglottogram data (http://sail.usc.edu/~kazemzad/EGG/only_source/ , this is a measurement of the opening and closing of the vocal folds from the USC phonetics lab) in Xmms and route it to the tweaked vowel synthesizer (http://sail.usc.edu/~kazemzad/EGG/clam/experimentalVowelSynth_from_EGG.clamnetwork , here I disconnected the harmonic peak generator and rerouted the peaks from the SMSAnalysisCore0 processing to the vowel resonator).  It sounds a bit more natural than the original, but this may be more b/c of the natural pitch and timing than the actual glottal pulse info (the EGG records the opening and closing, not the actual sound input to the vocal resonator).

Another thought for a possible improvement would be to reorient the f1 f2 space so it matches the typical vowel diagram (and maybe put such a diagram in the background).  This will probably involve real programming...

== Installing clam on Gentoo ==

(May 28)
I finally got clam working on my desktop.  Using Ubuntu on my desktop was easy, but with my desktop has given me some problems.  One problem was that I already had qt3 installed and the portage package manager didn't like trying to install two versions of a program.  I down loaded qt4 to install manually in a separate location and I had trouble building it.  It turns out there was some kind of typo in one of the defines in qglframebufferobject.cpp (cf. http://lists.kde.org/?l=kde-devel&m=115260405432346&w=2 ) .  Luckily, I found this kde mailing list post instead of having to figure it out myself.  Then I added /usr/local/Trolltech/Qt-4.2.3/lib to the PKG_CONFIG_PATH environment variable in ~/.bash_profile.  Also, I changed the QTDIR env var to /usr/local/Trolltech/Qt-4.2.3.  So far I've tested Annotator and NetworkEditor.  There is still some problems with audio playback but I'm not sure what it is... could be jack or sound card.  It seems to occur when there is a lot of load on the cpu.


== Project Planning ==

=== Week 1-2  (May 28 to Jun 11) ===
I decided to start working on a formant recognizer/extractor.  It seems that this could be used as a precursor to several other things, so it seems like a good place to start.  For one, it could be used with the vowel synthesizer as a kind of analysis/synthesis loop.  A further step could be to make a special formant recognizer for singing voice.  Another aspect that is close to my interest is user specific modeling of formants (b/c people have different formant locations based on their anatomy, native language/dialect, etc, even though perceptually we can distinguish which vowel is being said).  This could be applied to the gender change network since one of the main factors in formant variation is male/female.  (cf a  paper, Robust Gender Dependent Acoustic Phonetic Modelling in Continuous Speech Recognition based on a new Automatic Male/Female Classification by Vergin, Farhat, and O'Shaughnessy: approximately 90% accuracy automatically predicting male/female using f1 and f2).

==== todo ====

* check out some existing implementations (difficulty: 3, importance: 7)
** wavesurfer/snack/etropic xwaves, 
** praat, 
** Sandra Gilaberts thesis, 
** matlab speech tools, and 
** Rabiner's notes
** Xavier's chapter 10 in DAFX

* figure out how to create a processing (difficulty: 5, importance: 10 )

* implement best candidate from above existing implementations (difficulty: 7, importance: 10)
** get something working as soon as possible and submit it for review

==== notes ====

In the past two weeks I've refreshed my knowledge about the basics of the LPC method for formant extraction.  The books that I've looked into are Rabiner and Juang, Quatieri, Xavier's chapter in DAFX, and "Speech Processing and Synthesis Toolboxes" by D.G. Childers.  The last reference was one that I found in the bookshelf at my lab and it turned out to be the most helpful (chapter 5 on linear prediction).  The main part that was hard to dig out of the other books what was the relation between the LPC coefficients and the formant bandwidths.  I mainly focused on understanding the LPC model in detail, but reading ch. 10 in DAFX gave me some other ideas that start with the fft instead of LPC/autocorrelation.  Another idea is to have a complementary processing that uses a tube model to synthesize vowels (since the LPC formant extraction is based on a tube model interpretation of the vocal tract).  Sandra Gilabert's thesis was helpful, but it would have been more helpful in English :) It didn't go into the formant bandwidth measurement, like many of the other books I looked at.  This turned out to be fairly simple (probably intuitive to those who have a strong signal processing background).  The frequency of the formant is of course the arg of the complex conjugate LPC pole location (times 1/2*pi*samplingPeriod to convert to Hz) and the bandwidth is the absolute log of the LPC magnitude (times 1/pi*samplingPeriod).  I'd like to post a full derivation of all this stuff on the blog/rss feed so check back later if you're interested...

=== Week 3-4  (Jun 11 to Jun 25) ===

=== Week 5-6  (Jun 25 to Jul 9) ===

=== Week 7-8  (Jul 9 to Jul 23) ===
*mid term evaluation

=== Week 9-10  (Jul 23 to Aug 6) ===

=== Week 11-12  (Aug 6 to Aug 20) ===

=== Week 13 +  (Aug 20 +) ===
*final evaluation
*define continuing work
